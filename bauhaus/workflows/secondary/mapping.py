import os.path as op

# ----- Splitting, merging, consolidating -----

# N.B.: The name for subreads sets generated by dataset split is
#       currently {movieName}.subreadset.{chunk#}.xml, which Martin
#       and I think is wrong -- it should be
#       {movieName}.chunk.{subreadset#}.xml; Martin will fix this
#       soon.  When it is fixed, we don't need separate methods for
#       subreads and alignments.

def subreadsChunkName(datumFilename):
    """
    returns movieName or movieName.chunk#, depending on whehter chunked or not
    """
    base = op.basename(datumFilename)
    fields = base.split(".")
    assert len(fields) in (3, 4) and fields[-1] == "xml"
    movieName = fields[0]
    assert fields[-2].startswith("chunk")
    return "%s.%s" % (movieName, fields[-2])

def alignmentsChunkName(datumFilename):
    base = op.basename(datumFilename)
    fields = base.split(".")
    assert len(fields) in (3, 4) and fields[-1] == "xml"
    movieName = fields[0]
    assert fields[1].startswith("chunk")
    return "%s.%s" % (movieName, fields[1])

def movieName(filename):
    base = op.basename(filename)
    fields = base.split(".")
    assert len(fields) in (3, 4) and fields[-1] == "xml"
    movieName = fields[0]
    return movieName


def genSubreadsSetSplit(pflow, subreadsSet, splitFactor):
    # Split by ZMWs.  Returns: [dset]
    assert splitFactor >= 1
    pflow.genRuleOnce(
            "splitByZmw",
            "$grid dataset split --chunks %d --outdir $outdir $in" % (splitFactor,))
    movie = movieName(subreadsSet)
    splitOutputs =  [ "{condition}/subreads_chunked/%s.subreadset.chunk%d.xml" % (movie, i)
                      for i in xrange(splitFactor) ]
    pflow.genBuildStatement(splitOutputs,
                            "splitByZmw",
                            [subreadsSet],
                            variables={"outdir": "{condition}/subreads_chunked"})
    return splitOutputs


def genAlignmentSetMerge(pflow, alignmentSets):
    # MERGE entails the lightweight operation of merging the
    # dataset XML files--BAM files not merged
    pflow.genRuleOnce("mergeAlignmentSets",
                      "$grid dataset merge $out $in")
    outputs = [ "{condition}/mapping/{movieName}.alignmentset.xml" ]
    return pflow.genBuildStatement(outputs,
                                   "mergeAlignmentSets",
                                   alignmentSets)

def genAlignmentSetConsolidate(pflow, alignmentSets):
    # CONSOLIDATE entails actually merging BAM files
    pflow.genRuleOnce("consolidateAlignmentSets",
                      "$grid dataset consolidate $out $in")
    outputs = [ "{condition}/mapping/{movieName}.alignmentset.xml" ]
    return pflow.genBuildStatement(outputs,
                                   "consolidateAlignmentSets",
                                   alignmentSets)

# ----------- Mapping --------------------


# Assumption that should be asserted: each input subreads set comes from a separate movie.

def genMapping(pflow, subreadsSets, reference):
    """
    Map the subreads set, without chunking.  This is painfully slow on
    Sequel-scale data.
    """
    mapRule = pflow.genRuleOnce(
        "map",
        "$gridSMP $ncpus pbalign -j$ncpus $in $reference $out")
    for subreadsSet in subreadsSets:
        with pflow.context("movieName", movieName(subreadsSet)):
            buildVariables = dict(reference=reference, ncpus=8)
            pflow.genBuildStatement(["{condition}/mapping/{movieName}.alignmentset.xml"],
                                    "map",
                                    [subreadsSet],
                                    buildVariables)

def genChunkedMapping(pflow, subreadsSets, reference, splitFactor=8):
    """
    Break the subreads set into chunks, map the chunks, then
    consolidate the mapped chunks
    """
    mapRule = pflow.genRuleOnce(
        "map",
        "$gridSMP $ncpus pbalign -j$ncpus $in $reference $out")

    for subreadsSet in subreadsSets:
        with pflow.context("movieName", movieName(subreadsSet)):
            alignmentSetChunks = []
            subreadsSetChunks = genSubreadsSetSplit(pflow, subreadsSet, splitFactor)
            for (i, subreadsSetChunk) in enumerate(subreadsSetChunks):
                with pflow.context("chunkNum", i):
                    buildVariables = dict(reference=reference, ncpus=8)
                    buildStmt = pflow.genBuildStatement(["{condition}/mapping_chunks/{movieName}.chunk{chunkNum}.alignmentset.xml"],
                                                        "map",
                                                        [subreadsSetChunk],
                                                        buildVariables)
                    alignmentSetChunks.extend(buildStmt.outputs)
            genAlignmentSetConsolidate(pflow, alignmentSetChunks)


# -------------------- Demo -------------------

# lambda short insert ecoli runs intended for CCS analysis
if __name__ == '__main__':
    from bauhaus.pflow import PFlow
    inputDataByCondition = { "Replicate1" : ["/pbi/collections/315/3150128/r54008_20160308_001811/1_A01/m54008_160308_002050.subreadset.xml"],
                             "Replicate2" : ["/pbi/collections/315/3150128/r54008_20160308_001811/2_B01/m54008_160308_053311.subreadset.xml"] }

    reference = "/mnt/secondary/iSmrtanalysis/current/common/references/lambdaNEB/sequence/lambdaNEB.fasta"

    pflow = PFlow()
    for (condition, inputSubreadSets) in inputDataByCondition.iteritems():
        with pflow.context("condition", condition):
            #genMapping(pflow, inputSubreadSets, reference)
            genChunkedMapping(pflow, inputSubreadSets, reference, 8)
    pflow.write("build.ninja")
